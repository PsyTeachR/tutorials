[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"material tutorials governed CC-license. Feel free repurpose needs, please cite original ! source files can found https://github.com/psyteachr/tutorials.","code":""},{"path":"file-formats.html","id":"file-formats","chapter":"1 Importing data from different file formats into R","heading":"1 Importing data from different file formats into R","text":"Freda Wan (November 4, 2021)tutorial covers basic functions import data different formats access tibbles.","code":""},{"path":"file-formats.html","id":"at-a-glance","chapter":"1 Importing data from different file formats into R","heading":"1.1 At a glance","text":" ","code":""},{"path":"file-formats.html","id":"ref-csv","chapter":"1 Importing data from different file formats into R","heading":"1.2 1. CSV","text":"familiar importing common-separated values (CSV) files using read_csv() function offered Tidyverse's 'readr' package. data separated tabs, semicolons, characters \"|\"? helpful know similar functions 'readr' can readily help import data R.tab-separated files (.tsv), use read_tsv().semicolon-separated files, comma decimal point separator, use read_csv2().syntax.","code":"\n#library(tidyverse)\ntbl_csv <- read_csv(\"data/filename.csv\")\ntbl_csv2 <- read_csv2(\"data/filename.csv\")\ntbl_tsv <- read_tsv(\"data/filename.tsv\")"},{"path":"file-formats.html","id":"other-delimiters","chapter":"1 Importing data from different file formats into R","heading":"1.2.1 Other delimiters","text":"data delimited characters \"|\", use read_delim() specify delimiter.example, text file looks like dummy data , can see \"|\" delimiter. (try code , can copy text, save file name delim_data.txt.)","code":"ParticipantID|Condition1|Condition2|Condition3|Control\n130059284|0.4|0.01|0.2|0\n290100722|0.3|0.02|0.3|1\n387005398|0.5|0.01|0.4|0\ndat_delim <- read_delim(\"data/delim_data.txt\", delim = \"|\")  ## Rows: 3 Columns: 5## ── Column specification ─────────────────────────────────────────────────────────\n## Delimiter: \"|\"\n## dbl (5): ParticipantID, Condition1, Condition2, Condition3, Control## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"file-formats.html","id":"changing-the-data-type-for-each-column","chapter":"1 Importing data from different file formats into R","heading":"1.2.2 Changing the data type for each column","text":"Sometimes want specify data type column. load data, R generates message () indicate data type column. case, R recognised 5 columns doubles dbl, class numeric data, similar float programming languages.say want ParticipantID characters Control boolean (.e., TRUE FALSE, known logical R), can add col_types argument.code , col_types = \"c???l\" specifies 5 columns, want change data type ParticipantID character Control logical. Since need change data type Condition1, Condition2, Condition3, use ? allow R guess.can specify data type want using one character represent column. column types : c = character, = integer, n = number, d = double, l = logical, f = factor, D = date, T = date time, t = time, ? = guess, _/- skip column.argument col_types also used functions tutorial, including read_excel() Excel data read_table() reading text tibble.","code":"\ndat_delim_col_types <- read_delim(\"data/delim_data.txt\", delim = \"|\", col_types = \"c???l\")  \nspec(dat_delim_col_types) # check column data types## cols(\n##   ParticipantID = col_character(),\n##   Condition1 = col_double(),\n##   Condition2 = col_double(),\n##   Condition3 = col_double(),\n##   Control = col_logical()\n## )"},{"path":"file-formats.html","id":"write-data-to-csv","chapter":"1 Importing data from different file formats into R","heading":"1.2.3 Write data to CSV","text":"loading data various file formats cleaned wrangled data, may want save tibble .csv. allow view data later share file others without run whole R script .syntax.write_csv(cleaned_data, \"cleaned_data.csv\")Read 'readr' package see cheat sheet . Related packages include 'readxl' (detailed ) 'googlesheets4' allows read data write data Google Sheets. ","code":""},{"path":"file-formats.html","id":"ref-xls","chapter":"1 Importing data from different file formats into R","heading":"1.3 2. Excel (.xls or .xlsx)","text":"use 'readxl' package, part Tidyverse, read Excel data R. Try 'readxl' functions either using data download Demo.xlsx file (111KB). data adapted public domain dataset, Chocolate Bar Ratings (Tatman, 2017), contains expert ratings 1700 chocolate bars.demonstration purposes, demo file contains 3 sheets, sorted chocolate makers' company names starting G (Sheet 1), H Q (Sheet 2), R Z (Sheet 3). can access sheets either name index.check dat_excel (using either summary() base R dplyr::glimpse()), see specify sheet, first sheet loaded.Use 'readr' function excel_sheets() get list sheet names., can specify sheet want load, sheet name index.","code":"\n# Suggested: install 'readxl' and load separately\nlibrary(readxl) \ndat_excel<- readxl::read_excel(\"data/Demo.xlsx\") # by default, this loads Sheet 1 only. \nexcel_sheets(\"data/Demo.xlsx\")## [1] \"Companies_A_to_G\" \"Companies_H_to_Q\" \"Companies_R_to_Z\"\nsheet_by_name <- read_excel(\"data/Demo.xlsx\", sheet = \"Companies_H_to_Q\") #load sheet 2 by name\nsheet_by_index <- read_excel(\"data/Demo.xlsx\", sheet = 3) #load sheet by index"},{"path":"file-formats.html","id":"dealing-with-formulas-and-formatted-cells","chapter":"1 Importing data from different file formats into R","heading":"1.3.1 Dealing with formulas and formatted cells","text":"Sometimes may encounter Excel data contain formulas written VBA macros highlighted cells yellow bold.want extract formulas formatted cells Excel, R functions help. 'tidyxl' package function tidyxl::xlsx_cells() reads property cell Excel spreadsheet, data type, formatting, whether formula, cell's font, height width. Please refer 'tidyxl' vignette.However, proceed caution! Spreadsheet errors, either due human mistakes Excel's autocorrect functions, raised reproducibility concerns behavioural science genetics research (see Alexander, 2013; Lewis, 2021). see strange behaviour Excel data, check file: formulas referring correct cells? large number autocorrected date? doubt, open file Excel check. ","code":""},{"path":"file-formats.html","id":"ref-txt","chapter":"1 Importing data from different file formats into R","heading":"1.4 3. Plain text (.txt)","text":"data .txt files, can either use readLines() base readr::read_lines() Tidyverse. , use latter, runs faster large datasets.example, use simple dataset Stigliani Grill-Spector (2018), neuroscience study. can click download data directly (392 bytes).First, read file see contains.n_max argument specifies many lines data like read. can see need skip first 3 lines data form tibble. use read_table() create tibble.default, argument col_names set TRUE, first row text input imported column names. set FALSE, column names generated automatically X1, X2, X3, etc.want rename columns line code, can enter character vector col_names. example:course, another way rename columns using dplyr::rename(data, new_name = old_column_name) dplyr::rename_with(data, function). example, running following turn column names upper case. Try see. ","code":"\n#load the tidyverse package\nreadr_text <- read_lines((\"data/Exp1_Run1.txt\"), n_max=10)\nreadr_text##  [1] \"Exp1 conditions: 2s, 4s, 8s, 15s, 30s\"\n##  [2] \"Run duration (s): 276\"                \n##  [3] \"\"                                     \n##  [4] \"Trial Condition Onset Duration Image\" \n##  [5] \"1 8s 12 8 scrambled-260.jpg\"          \n##  [6] \"2 2s 32 2 scrambled-191.jpg\"          \n##  [7] \"3 4s 46 4 scrambled-481.jpg\"          \n##  [8] \"4 30s 75 30 scrambled-996.jpg\"        \n##  [9] \"5 15s 117 15 scrambled-355.jpg\"       \n## [10] \"6 8s 144 8 scrambled-47.jpg\"\ndat_txt <- read_table(file = \"data/Exp1_Run1.txt\", skip = 3) # skip first 3 lines\ndat_txt_new_column_names <- read_table(file = \"data/Exp1_Run1.txt\", skip = 4, col_names = c(\"Trial_new\", \"Condition_new\", \"Onset_new\", \"Duration_new\", \"Image_new\"))\n#since we are renaming the columns, we skip 4 lines\ndat_txt_upper <- rename_with(dat_txt, toupper)"},{"path":"file-formats.html","id":"ref-spss","chapter":"1 Importing data from different file formats into R","heading":"1.5 4. SPSS data (.sav)","text":"use 'haven' package, part Tidyverse, import SPSS data tibble.example uses SPSS data Norman et al.(2021), Study 2, examines adult identity. can click download data directly (564KB).tibble dat_sav 173 columns 658 rows. need load subset columns, col_select argument allows select columns index column name. example using col_select output looks like. alternative dplyr::select(). ","code":"\n# Suggested: install the package 'haven' and load it in addition to Tidyverse. \nlibrary(haven)\ndat_sav <- haven::read_sav(\"data/EA Across Age Data.sav\")\n#load the first 8 columns\ndat_select_by_index <- read_sav(\"data/EA Across Age Data.sav\", col_select=(1:8))\n#load columns with name starting with \"IDEA\"\ndat_select_by_colname <- read_sav(\"data/EA Across Age Data.sav\", \n                                  col_select = starts_with(\"IDEA\"))"},{"path":"file-formats.html","id":"related","chapter":"1 Importing data from different file formats into R","heading":"1.5.0.1 Related","text":"encounter .por files, ASCII text data files generated SPSS, use haven::read_por() function.'haven' package can also read Stata SAS files R. Read  ","code":""},{"path":"file-formats.html","id":"ref-rds","chapter":"1 Importing data from different file formats into R","heading":"1.6 5. Binary R data (.rds)","text":"RDS files store datasets compressed format save storage space. Additionally, RDS preserves data types dates factors, need worry redefining data types reading file R.read .rds files, use either readRDS() baseR read_rds() Tidyverse's 'readr' package. use Tidyverse example .example uses data Lukavsky (2018), Experiment 1. study investigates participants' ability recognise seen central peripheral vision. can click download data directly (185KB).see dataset 5300 rows 26 columns. first 6 lines look like. ","code":"\n#library(tidyverse)\ndat_binary <- read_rds(\"data/exp1.rds\") "},{"path":"file-formats.html","id":"ref-json","chapter":"1 Importing data from different file formats into R","heading":"1.7 6. JSON","text":"JSON files store nested lists data tree-like structure. use 'jsonlite' package view access data R.can download example.json file (4KB). data sourced International Union Conservation Nature Red List Threatened Species.can also navigate data using names() base simply type dat_json$\"Panthera leo\". dollar sign $ refers variable column. RStudio, type data_object_name$, available variables columns shown choice.Use as_tibble() put data tibble processing.can transpose tibble easier read. ","code":"\n# install the 'jsonlite' package first\nlibrary(jsonlite)\ndat_json <- fromJSON(txt=\"data/example.json\", simplifyDataFrame = FALSE)\nnames(dat_json) #gets names of what's in the object\nnames(dat_json$`Panthera tigris`) # get variable names one level down. Use `` or \"\" for variable names containing spaces. ## [1] \"Panthera leo\"    \"Panthera tigris\" \"Canis simensis\"  \"Canis rufus\"    \n##  [1] \"common name\"                   \"population trend\"             \n##  [3] \"number of mature individuals\"  \"system\"                       \n##  [5] \"generation length, in years\"   \"habitat type\"                 \n##  [7] \"Extant in\"                     \"threats\"                      \n##  [9] \"conservation actions in place\" \"link\"\ntiger_conservation <- dat_json$`Panthera tigris`$`conservation actions in place` %>% as_tibble()\ntiger_conversation_long_view <- tiger_conservation %>% pivot_longer(cols = everything())"},{"path":"file-formats.html","id":"reference","chapter":"1 Importing data from different file formats into R","heading":"1.8 Reference","text":"Alexander, R. (2013, April 20). Reinhart, Rogoff... Herndon: student caught profs. BBC. https://www.bbc.com/news/magazine-22223190International Union Conservation Nature. (n.d.). IUCN Red List Threatened Species. https://www.iucnredlist.orgLewis, D. (2021, August 25). Autocorrect errors Excel still creating genomics headache. Nature. https://www.nature.com/articles/d41586-021-02211-4Lukavsky, J. (2018, December 5). Scene categorization presence distractor. Retrieved osf.io/849wmNorman, K., Hernandez, L., & Obeid, R. (2021, January 12). Study 2. ? Gender Differences Identity Exploration Transition Adulthood. Retrieved osf.io/agfvzStigliani, ., & Grill-Spector, K. (2018, July 5). Temporal Channels. https://doi.org/10.17605/OSF.IO/MW5PKTatman, R. (2017). Chocolate Bar Ratings. Kaggle Datasets. https://www.kaggle.com/rtatman/chocolate-bar-ratings","code":""},{"path":"multiple-import.html","id":"multiple-import","chapter":"2 Importing data from multiple files","heading":"2 Importing data from multiple files","text":"Dale Barr (October 30, 2019)Sometimes multiple files, want read one big table analysis. following code allows read whole bunch files subdirectory.call subdirectory files live datadir; subdirectory may different name. files subdirectory, directory script, replace datadir full stop, .e., dir(\".\", \"\\\\.[Cc][Ss][Vv]$\").main work accomplished using map() function purrr package, part tidyverse. function map() applies function (case, read_csv()) elements vector supplied first argument.want run example see things work, need set environment sample files.example, filename variable todo contains values function read_csv() applied. argument col_types supplied map() intended passed along read_csv(). additional arguments want pass along read_csv() can placed (e.g., skip = 1).preprocessing need file reading , can write function call place read_csv().Sometimes filename contains metadata (e.g., string identifying subject: S01, S02 etc) want pull . can creating new variable using mutate() filename. example , strip away path filename (datadir/S01.csv S01.csv) using basename() remove file extension .csv using tools::file_path_sans_ext().","code":"\nlibrary(\"tidyverse\")\n\ndir.create(\"datadir\", FALSE)\n\n## create fake data files for three subjects\nwrite_csv(iris %>% slice(1:5), file.path(\"datadir\", \"S01.csv\"))\nwrite_csv(iris %>% slice(6:10), file.path(\"datadir\", \"S02.csv\"))\nwrite_csv(iris %>% slice(11:15), file.path(\"datadir\", \"S03.csv\"))\nlibrary(\"tidyverse\")\n\n# \"\\\\.csv$\" = find all files ending with csv or CSV\ntodo <- tibble(filename = dir(\"datadir\", \"\\\\.[Cc][Ss][Vv]$\", full.names = TRUE))\n\nall_data <- todo %>%\n  mutate(imported = map(filename, read_csv, col_types = \"ddddc\")) %>%\n  unnest(cols = c(imported))\n\nall_data\nall_data_id <- all_data %>%\n  mutate(subj_id = basename(filename) %>% tools::file_path_sans_ext()) %>%\n  select(subj_id, Sepal.Length:Species)\n\nall_data_id"},{"path":"multi-row-headers.html","id":"multi-row-headers","chapter":"3 Multi-Row Headers","heading":"3 Multi-Row Headers","text":"Lisa DeBruine (2021-10-17)student help forum asked help making wide-format dataset long. tried load data, realised first three rows header rows. code wrote deal .First, make small CSV \"file\" . typical case, read data file.try read data, get message duplicate column names resulting table \"fixed\" column headers next two columns headers first two rows.Instead, read just header rows setting n_max equal number header rows col_names FALSE.get table looks like :can make new header names pasting together names three rows summarising across columns paste() function collapsing using \"_\". Use unlist() unname() convert result table vector.Now can read data without three header rows. Use skip skip headers set col_names new names.excel file merges duplicate headers across rows, little trickier, still -able.first steps : read first three rows.code starts second column fills missing data value previous column.Now can continue generating pasted name .data set multiple headers, probably want change shape data. quick example use pivot_longer() pivot_wider() variable names like .","code":"\n# required packages\nlibrary(tidyverse)\nlibrary(readxl)\ndemo_csv <- I(\"SUB1, SUB1, SUB1, SUB1, SUB2, SUB2, SUB2, SUB2\nCOND1, COND1, COND2, COND2, COND1, COND1, COND2, COND2\nX, Y, X, Y, X, Y, X, Y\n10, 15, 6, 2, 42, 4, 32, 5\n4, 43, 7, 34, 56, 43, 2, 33\n77, 12, 14, 75, 36, 85, 3, 2\")\ndata <- read_csv(demo_csv)\ndata_head <- read_csv(demo_csv, \n                      n_max = 3, \n                      col_names = FALSE)\nnew_names <- data_head %>%\n  summarise(across(.fns = paste, collapse = \"_\")) %>%\n  unlist() %>% unname()\n\nnew_names## [1] \"SUB1_COND1_X\" \"SUB1_COND1_Y\" \"SUB1_COND2_X\" \"SUB1_COND2_Y\" \"SUB2_COND1_X\"\n## [6] \"SUB2_COND1_Y\" \"SUB2_COND2_X\" \"SUB2_COND2_Y\"\ndata <- read_csv(demo_csv, \n                 skip = 3, \n                 col_names = new_names,\n                 show_col_types = FALSE)\ndata_head <- read_excel(\"data/3headers_demo.xlsx\",\n                        n_max = 3, \n                        col_names = FALSE)\nfor (i in 2:ncol(data_head)) {\n  prev <- data_head[, i-1]\n  this <- data_head[, i]\n  missing <- is.na(this)\n  this[missing, ] <- prev[missing, ]\n  data_head[, i] <- this\n}\nnew_names <- data_head %>%\n  summarise(across(.fns = paste, collapse = \"_\")) %>%\n  unlist() %>% unname()\n\nnew_names## [1] \"SUB1_COND1_X\" \"SUB1_COND1_Y\" \"SUB1_COND2_X\" \"SUB1_COND2_Y\" \"SUB2_COND1_X\"\n## [6] \"SUB2_COND1_Y\" \"SUB2_COND2_X\" \"SUB2_COND2_Y\"\ndata <- read_excel(\"data/3headers_demo.xlsx\", \n                   skip = 3, \n                   col_names = new_names)\n\ndata_long <- data %>%\n  # add a row ID column if one doesn't exist already\n  mutate(trial_id = row_number()) %>%\n  # make a row for each data column\n  pivot_longer(\n    cols = -trial_id, # everything except trial_id\n    names_to = c(\"sub_id\", \"condition\", \"coord\"),\n    names_sep = \"_\",\n    values_to = \"val\"\n  ) %>%\n  # make x and y coord columns\n  pivot_wider(\n    names_from = coord,\n    values_from = val\n  )"},{"path":"detecting-runs-in-a-sequence.html","id":"detecting-runs-in-a-sequence","chapter":"4 Detecting \"runs\" in a sequence","heading":"4 Detecting \"runs\" in a sequence","text":"Dale Barr (October 30, 2019)say table like , want find start end frames run Z amidst , b, c, d. code sets kind situation. worry understand code; just run create example data runsdata, look table.say want find start stop frames Z appears stimulus, independently combination subject trial. stimulus looks subject 1 trial 1.can see first run Zs frame 11 13, 28 second 26 28. want write function processes data trial results table like :first thing add logical vector tibble whose value TRUE target value (e.g., Z) present sequence, false otherwise.want iterate subjects trials. start creating tibble columns is_target nested column called subtbl.want iterate little subtables stored within subtbl row table, passing table function find runs return another table, store new column. write function detect runs. function need function rle() (Run-Length Encoding) base R. run logical vector created (is_target). creating function, see rle() values is_target subject 1, trial 1.make sense, look help rle() (type ?rle console). Now ready write function, detect_runs().can test function s1t1 just make sure works.OK, now ready run function.Now just unnest done!","code":"\nlibrary(\"tidyverse\")\n\ncreate_run_vec <- function() {\n  ## create a random string of letters with two runs\n  c(rep(sample(letters[1:4]), sample(2:4, 4, TRUE)),\n               rep(\"Z\", 3),\n               rep(sample(letters[1:4]), sample(2:4, 4, TRUE)),\n               rep(\"Z\", 3),\n               rep(sample(letters[1:4], 2), sample(2:4, 2, TRUE)))\n}\n\n## 5 subjects, 3 trials each\nrunsdata <- tibble(\n  subject = rep(1:5, each = 3),\n  trial = rep(1:3, 5),\n  stimulus = rerun(15, create_run_vec())) %>%\n  unnest(stimulus) %>%\n  group_by(subject, trial) %>%\n  ungroup() %>%\n  select(subject, trial, stimulus)##  [1] \"b\" \"b\" \"b\" \"d\" \"d\" \"d\" \"c\" \"c\" \"a\" \"a\" \"Z\" \"Z\" \"Z\" \"d\" \"d\" \"d\" \"a\" \"a\" \"a\"\n## [20] \"a\" \"c\" \"c\" \"c\" \"b\" \"b\" \"Z\" \"Z\" \"Z\" \"c\" \"c\" \"c\" \"c\" \"b\" \"b\" \"b\"\nrunsdata_tgt <- runsdata %>%\n  mutate(is_target = (stimulus == \"Z\"))\n\nhead(runsdata_tgt)\nruns_nest <- runsdata_tgt %>%\n  select(-stimulus) %>% # don't need it anymore\n  nest(subtbl = c(is_target))\ns1t1 <- runsdata_tgt %>% filter(subject == 1L, trial == 1L) %>% pull(is_target)\n\ns1t1\n\nrle(s1t1)##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n## [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## Run Length Encoding\n##   lengths: int [1:5] 10 3 12 3 7\n##   values : logi [1:5] FALSE TRUE FALSE TRUE FALSE\ndetect_runs <- function(x) {  \n  if (!is.logical(x[[1]])) stop(\"'x' must be a tibble whose first column is of type 'logical'\")\n  runs <- rle(x[[1]])\n  run_start_fr <- c(1L, cumsum(runs$lengths[-length(runs$lengths)]) + 1L)\n  run_end_fr <- run_start_fr + (runs$lengths - 1L)\n  \n  tgt_start <- run_start_fr[runs$values]\n  tgt_end <- run_end_fr[runs$value]\n  tibble(run = seq_along(tgt_start),\n         start_fr = tgt_start,\n         end_fr = tgt_end)\n}\ndetect_runs(tibble(lvec = s1t1))\nresult <- runs_nest %>%\n  mutate(runstbl = map(subtbl, detect_runs))\n\nhead(result)\ndata <- result %>%\n  select(-subtbl) %>%\n  unnest(runstbl)\n\nhead(data)"},{"path":"pipes.html","id":"pipes","chapter":"5 Pipes","heading":"5 Pipes","text":"Lisa DeBruine (2018-12-22)Pipes way order code readable format.say small data table 10 participant IDs, two columns variable type , 2 columns variable type B. want calculate mean variables mean B variables return table 10 rows (1 participant) 3 columns (id, A_mean B_mean).One way creating new object every step using object next step. pretty clear, created 6 unnecessary data objects environment. can get confusing long scripts.\ncan name object data keep replacing old data object new one step. keep environment clean, recommend makes easy accidentally run code order running line--line development debugging.\nOne way avoid extra objects nest functions, literally replacing data object code generated previous step. can fine short chains.gets extremely confusing long chains:pipe lets \"pipe\" result function next function, allowing put code logical order without creating many extra objects.can read code top bottom follows:Make tibble called data \nid 1 10,\nA1 10 random numbers normal distribution mean 0,\nA2 10 random numbers normal distribution mean 1,\nB1 10 random numbers normal distribution mean 2,\nB2 10 random numbers normal distribution mean 3; \nid 1 10,A1 10 random numbers normal distribution mean 0,A2 10 random numbers normal distribution mean 1,B1 10 random numbers normal distribution mean 2,B2 10 random numbers normal distribution mean 3; thenGather create variable value column columns A_1 B_2; thenSeparate column variable 2 new columns called varand var_n, separate character 1; thenGroup columns id var; thenSummarise new column called mean mean value column group, drop groups ; thenSpread make new columns key names var values mean; thenRename make columns called A_mean (old ) B_mean (old B)can make intermediate objects whenever need break code getting complicated need debug something.\ncan debug pipe running just first functions highlighting beginning just pipe want stop . Try highlighting data <- end separate function typing cmd-return. data look like now?\n","code":"\nlibrary(tidyverse)\n# make a data table with 10 subjects\ndata_original <- tibble(\n  id = 1:10,\n  A1 = rnorm(10, 0),\n  A2 = rnorm(10, 1),\n  B1 = rnorm(10, 2),\n  B2 = rnorm(10, 3)\n)\n\n# gather columns A1 to B2 into \"variable\" and \"value\" columns\ndata_gathered <- gather(data_original, variable, value, A1:B2)\n\n# separate the variable column at the _ into \"var\" and \"var_n\" columns\ndata_separated <- separate(data_gathered, variable, c(\"var\", \"var_n\"), sep = 1)\n\n# group the data by id and var\ndata_grouped <- group_by(data_separated, id, var)\n\n# calculate the mean value for each id/var \ndata_summarised <- summarise(data_grouped, mean = mean(value), .groups = \"drop\")\n\n# spread the mean column into A and B columns\ndata_spread <- spread(data_summarised, var, mean)\n\n# rename A and B to A_mean and B_mean\ndata <- rename(data_spread, A_mean = A, B_mean = B)\nmean_petal_width <- round(mean(iris$Petal.Width), 2)\n# do not ever do this!!\ndata <- rename(\n  spread(\n    summarise(\n      group_by(\n        separate(\n          gather(\n            tibble(\n              id = 1:10,\n              A1 = rnorm(10, 0),\n              A2 = rnorm(10, 1),\n              B1 = rnorm(10, 2),\n              B2 = rnorm(10, 3)), \n            variable, value, A1:B2), \n          variable, c(\"var\", \"var_n\"), sep = 1), \n        id, var), \n      mean = mean(value),\n      .groups = \"drop\"), \n    var, mean), \n  A_mean = A, B_mean = B)\n# calculate mean of A and B variables for each participant\ndata <- tibble(\n  id = 1:10,\n  A1 = rnorm(10, 0),\n  A2 = rnorm(10, 1),\n  B1 = rnorm(10, 2),\n  B2 = rnorm(10, 3)\n) %>%\n  gather(variable, value, A1:B2) %>%\n  separate(variable, c(\"var\", \"var_n\"), sep=1) %>%\n  group_by(id, var) %>%\n  summarise(mean = mean(value),\n            .groups = \"drop\") %>%\n  spread(var, mean) %>%\n  rename(A_mean = A, B_mean = B)"},{"path":"converting-strings-to-numbers.html","id":"converting-strings-to-numbers","chapter":"6 Converting strings to numbers","heading":"6 Converting strings to numbers","text":"Dale Barr (August 18, 2021)common situation want convert string values (\"Almost Always\", \"Frequently\") numeric values (5, 4, etc) can calculate score.solution present use something called \"lookup table\" matches string values numbers.First, assume following (made-) questionnaire asks four questions hygiene. bathe, often :Wash legs?Wash behind ears?Wash toes?Shampoo hair?assume answers scale made values \"Never\", \"Sometimes\", \"Frequently\", \"Always\", want assign scores 0, 1, 2, 3 respectively.(also made-) data five participants, stored tibble named dat.(want make dat can follow along example running code R, click button .)data wide format: single row participant data question forming columns. going first convert data wide long using pivot_longer(). (see minute.)Take look dat_long.Now one variable need convert numeric values (response) instead original four. easy solution: create \"lookup table\" mapping string values numeric values like .first check unique string values data. lookup table must match exactly approach work.step important sometimes fields can special characters normally see print table. distinct() %>% pull() pattern give values way makes visible. instance, student values data like :lookup table constantly failing lookup table created \\n middle string. Computers literal!OK now ready create lookup table match four values numbers.final step, inner_join() original table dat_long lookup variable response.IMPORTANT: check make sure join worked intended. values lookup table must exactly match values response column dat_long. easy make typo lookup table, values lost. easy test make sure number rows joined matches number rows dat_long.function stopifnot() make script fail stated condition (tables number rows) satisfied.Uh oh. Running gives Error: nrow(joined) == nrow(dat_long) TRUE. test failed, deliberately included typo lookup table. Can see ?always Always. Capitalization matters!fix lookup table good go. full code demonstration:useto calculate score subject.","code":"\nlibrary(\"tidyverse\")\n\ndat <- tribble(\n  ~subj_id, ~wash_legs,   ~wash_ears,   ~wash_toes,   ~shampoo,\n  \"S01\",    \"Sometimes\",  \"Never\",      \"Never\",      \"Frequently\",\n  \"S02\",    \"Sometimes\",  \"Frequently\", \"Frequently\", \"Always\",\n  \"S03\",    \"Never\",      \"Never\",      \"Never\",      \"Frequently\",\n  \"S04\",    \"Always\",     \"Always\",     \"Sometimes\",  \"Always\",\n  \"S05\",    \"Frequently\", \"Sometimes\",  \"Never\",      \"Sometimes\")\ndat_long <- dat %>%\n  pivot_longer(cols = wash_legs:shampoo,\n               names_to = \"question\", values_to = \"response\")\ndat_long %>%\n  distinct(response) %>%\n  pull()## [1] \"Sometimes\"  \"Never\"      \"Frequently\" \"Always\"[1] \"Somewhat\\nInfrequently\" \"Somewhat\\nFrequently\"   \"Very\\nInfrequently\"\n[4] \"Almost\\nNever\"          \"Very\\nFrequently\"       \"Almost\\nAlways\"\nlookup <- tribble(\n  ~response, ~score,\n  \"Never\",      0,\n  \"Sometimes\",  1,\n  \"Frequently\", 2,\n  \"always\",     3)\njoined <- inner_join(dat_long, lookup, by = \"response\")\nstopifnot(nrow(joined) == nrow(dat_long))\ndat_long <- dat %>%\n  pivot_longer(cols = wash_legs:shampoo,\n               names_to = \"question\", values_to = \"response\")\n\n## check for hidden values\ndat_long %>%\n  distinct(response) %>%\n  pull()\n\nlookup <- tribble(\n  ~response, ~score,\n  \"Never\",      0,\n  \"Sometimes\",  1,\n  \"Frequently\", 2,\n  \"Always\",     3)\n\njoined <- inner_join(dat_long, lookup, by = \"response\")\n\n## test whether the number of rows match\nstopifnot(nrow(joined) == nrow(dat_long))\njoined %>%\n  group_by(subj_id) %>%\n  summarise(hygiene = sum(score))"},{"path":"reverse-scoring.html","id":"reverse-scoring","chapter":"7 Reverse scoring","heading":"7 Reverse scoring","text":"Dale Barr (January 2021)Sometimes necessary reverse scores given questionnaire items. instance, scale Dog Appreciation Scale (DAS) [just made ] might items 'strongly agree' associated appreciating dogs (given highest score) items associated loathing (given lowest).say DAS scale six items measuring dog appreciation. People respond six items 5 point likert scale, 1=strongly disagree, 2=somewhat disagree, 3=neutral, 4=somewhat agree, 5=strongly agree.\nTable 7.1: Dog Appreciation Scale\nmade-questionnaire data 6 items 3 subjects, contained tibble named das. want reverse score items \"Cats better dogs\", \"Dogs noisy\", \"Dogs much responsibility\" summing total subject.First, assume data long format, like table . , please see materials reshaping wide long, section MSC book.going use programming trick call \"N-plus-one-minus-X trick\" score items need reverse coded. trick work whenever scale N scale points goes integer steps 1 N (e.g., 1, 2, 3, 4, 5). subtract Xs (observed score) N+1 get reversed value.newscore = (number_of_scale_points + 1) - oldscoreSo 5 point scale, :newscore = 6 - oldscoreand 7 point scale isnewscore = 8 - oldscore.can see works using following code:Note: scale goes 0 N, use N - X rather (N + 1) - X reverse score.can see already need something like:items need reverse scored. if_else() comes . , better said, if_else() comes %% (can pardon bit R humor).code adds new variable newscore result if_else() command stores resulting table das_coded. command following syntax:if_else(condition, value_if_true, value_if_false)., current value item found within vector options (%% operator ), first expression evaluates TRUE, 6-score returned; first expression evaluates FALSE, score returned.whenever recode score variable, ALWAYS check code correct, typos likely. best way run little test console. can just print data das_coded, lot data, use distinct() look check distinct values observed data.can see \"Cats better dogs\" \"Dogs noisy\" successfully reverse scored. can also see items forward scored, e.g., \"like dogs\", indeed forward scored (scores change).reverse scoring \"Dogs much responsibility\" failed. Can see problem code (hint: typo).responsibility mistyped responsibilitiySo correct code :done ! Now can proceed analyze data .","code":"\nlibrary(\"tidyverse\")\n\ndas <- tribble(\n  ~subj_id, ~item, ~score,\n  \"S01\", \"I like dogs\",                      5,\n  \"S01\", \"Dogs are fun\",                     5,\n  \"S01\", \"Cats are better than dogs\",        1,\n  \"S01\", \"Dogs are helpful\",                 4,\n  \"S01\", \"Dogs are too noisy\",               2,\n  \"S01\", \"Dogs are too much responsibility\", 2,\n  \"S02\", \"I like dogs\",                      3,\n  \"S02\", \"Dogs are fun\",                     4,\n  \"S02\", \"Cats are better than dogs\",        2,\n  \"S02\", \"Dogs are helpful\",                 4,\n  \"S02\", \"Dogs are too noisy\",               3,\n  \"S02\", \"Dogs are too much responsibility\", 5,\n  \"S03\", \"I like dogs\",                      1,\n  \"S03\", \"Dogs are fun\",                     3,\n  \"S03\", \"Cats are better than dogs\",        5,\n  \"S03\", \"Dogs are helpful\",                 2,\n  \"S03\", \"Dogs are too noisy\",               4,\n  \"S03\", \"Dogs are too much responsibility\", 5)\noldscores <- 1:5\nnewscores <- 6 - oldscores\n\nrbind(oldscores, newscores)##           [,1] [,2] [,3] [,4] [,5]\n## oldscores    1    2    3    4    5\n## newscores    5    4    3    2    1\ndas %>%\n  mutate(newscore = 6 - score)\ndas_coded <- das %>%\n  mutate(newscore = if_else(item %in% c(\"Cats are better than dogs\",\n                                        \"Dogs are too noisy\",\n                                        \"Dogs are too much responsibilitiy\"),\n                            6 - score,\n                            score))\ndas_coded %>%\n  distinct(item, score, newscore) %>%\n  print(n = +Inf) ## this makes sure *all* rows are printed, not just the first 20## # A tibble: 16 × 3\n##    item                             score newscore\n##    <chr>                            <dbl>    <dbl>\n##  1 I like dogs                          5        5\n##  2 Dogs are fun                         5        5\n##  3 Cats are better than dogs            1        5\n##  4 Dogs are helpful                     4        4\n##  5 Dogs are too noisy                   2        4\n##  6 Dogs are too much responsibility     2        2\n##  7 I like dogs                          3        3\n##  8 Dogs are fun                         4        4\n##  9 Cats are better than dogs            2        4\n## 10 Dogs are too noisy                   3        3\n## 11 Dogs are too much responsibility     5        5\n## 12 I like dogs                          1        1\n## 13 Dogs are fun                         3        3\n## 14 Cats are better than dogs            5        1\n## 15 Dogs are helpful                     2        2\n## 16 Dogs are too noisy                   4        2\ndas_coded <- das %>%\n  mutate(newscore = if_else(item %in% c(\"Cats are better than dogs\",\n                                        \"Dogs are too noisy\",\n                                        \"Dogs are too much responsibility\"),\n                            6 - score,\n                            score))\ndas_coded %>%\n  distinct(item, score, newscore) %>%\n  print(n = +Inf) ## this makes sure *all* rows are printed, not just the first 20## # A tibble: 16 × 3\n##    item                             score newscore\n##    <chr>                            <dbl>    <dbl>\n##  1 I like dogs                          5        5\n##  2 Dogs are fun                         5        5\n##  3 Cats are better than dogs            1        5\n##  4 Dogs are helpful                     4        4\n##  5 Dogs are too noisy                   2        4\n##  6 Dogs are too much responsibility     2        4\n##  7 I like dogs                          3        3\n##  8 Dogs are fun                         4        4\n##  9 Cats are better than dogs            2        4\n## 10 Dogs are too noisy                   3        3\n## 11 Dogs are too much responsibility     5        1\n## 12 I like dogs                          1        1\n## 13 Dogs are fun                         3        3\n## 14 Cats are better than dogs            5        1\n## 15 Dogs are helpful                     2        2\n## 16 Dogs are too noisy                   4        2"},{"path":"highlight-a-range-of-x-values.html","id":"highlight-a-range-of-x-values","chapter":"8 Highlight a range of x-values","heading":"8 Highlight a range of x-values","text":"Dale Barr (March 23, 2020)Sometimes want highlight particular range values; example, particular period time time series.code used create following plot.\nFigure 8.1: time series x = 40-60 highlighted\n","code":"\nlibrary(\"tidyverse\")\n\n## make up some example data\nexdata <- tibble(x = rep(1:100, 2),\n                 series = rep(1:2, each = 100),\n                 y = rnorm(200) + rep(c(30, 50), each = 100))\n\n## region we want to highlight\nregions <- tibble(x1 = 40, x2 = 60, y1 = -Inf, y2 = +Inf)\n\nggplot(exdata, aes(x, y)) +\n  geom_rect(data = regions,\n            inherit.aes = FALSE,\n            mapping = aes(xmin = x1, xmax = x2,\n                          ymin = y1, ymax = y2),\n            color = \"transparent\",\n            fill = \"blue\",\n            alpha = .2) +\n  geom_line(aes(group = series))"},{"path":"testing-for-normality.html","id":"testing-for-normality","chapter":"9 Testing for normality","heading":"9 Testing for normality","text":"Lisa DeBruine (2021-02-01)probably directed asked someone test normality predictors analysis. However, statistical tests like t-tests, ANOVAs, GLM-based tests assume residuals normally distributed matter predictors even dependent variable .blog post, going use data simulation show can visualise normality residuals QQ plots. going simulate data totally hypothetical population ferrets cats. going try predict energy levels pets weight. limited experience, tiny ferrets way energetic big ferrets. know nothing cats.Tiny, energetic Darwin big, lazy brother, Oy","code":"\nlibrary(tidyverse) # for data wrangling\nlibrary(faux)      # for data simulation\nlibrary(afex)      # for anova\nlibrary(cowplot)   # for dataviz\nset.seed(8675309)  # to make sure simulation values don't vary between runs"},{"path":"testing-for-normality.html","id":"simulate-data","chapter":"9 Testing for normality","heading":"9.1 Simulate Data","text":"use faux simulate data based data parameters like means, SDs correlations group. moment, faux can simulate multivariate normal distributions can convert distributions. simulate weights normal distribution mean 0 SD 1, convert uniform distribution pet type based ranges found online. Energy simulated normal distributions different means SDs cats ferrets. Energy uncorrelated weight cats negatively correlated ferrets.N.B. used simulating data using model parameters, way might make sense , often difficult figure parameters already pilot data.weight bimodal made two uniform distributions, energy bimodal made two normal distributions.\nFigure 9.1: Distibutions overall within species.\nrun Shapiro-Wilk test variables, conclude definitely normally distributed, matter !","code":"\ndata <- faux::sim_design(\n  within = list(vars = c(\"weight\", \"energy\")),\n  between = list(species = c(\"cat\", \"ferret\")),\n  n = 50,\n  mu = list(weight = c(cat = 0, ferret = 0),\n            energy = c(cat = 50, energy = 100)),\n  sd = list(weight = c(cat = 1, ferret = 1),\n            energy = c(cat = 15, energy = 20)),\n  r = list(cat = 0, ferret = -0.5),\n  plot = FALSE\n) %>%\n  mutate(weight = case_when(\n    species == \"cat\" ~ norm2unif(weight, 3.6, 4.5),\n    species == \"ferret\" ~ norm2unif(weight, 0.7, 2.0)\n  ))\nn <- 50\n\n# values approximated from an lm analysis\nb_0  <-  92 # intercept\nb_w  <- -13 # fixed effect of weight\nb_s  <-  85 # fixed effect of species\nb_ws <- -26 # weight*species interaction\nerr_sd <- 16 # SD of error term\n\n# simulate populations of cats and ferrets \n# with weights from uniform distributions\ncat <- data.frame(\n  id = paste0(\"C\", 1:n),\n  species = \"cat\",\n  weight = runif(n, 3.6, 4.5)\n)\n\nferret <- data.frame(\n  id = paste0(\"F\", 1:n),\n  species = \"ferret\",\n  weight = runif(n, 0.7, 2.0)\n)\n\n# join data and calculate DV based on GLM\ndata <- bind_rows(cat, ferret) %>%\n  mutate(\n    # effect-code species\n    species.e = recode(species, cat = -0.5, ferret = 0.5),\n    # simulate error term\n    err = rnorm(2*n, 0, err_sd),\n    # calculate DV\n    energy = b_0 + species.e*b_s + weight*b_w + \n             species.e*weight*b_ws + err\n  )\nshapiro.test(data$energy)## \n##  Shapiro-Wilk normality test\n## \n## data:  data$energy\n## W = 0.95486, p-value = 0.001759\nshapiro.test(data$weight)## \n##  Shapiro-Wilk normality test\n## \n## data:  data$weight\n## W = 0.82694, p-value = 1.821e-09"},{"path":"testing-for-normality.html","id":"calculate-residuals","chapter":"9 Testing for normality","heading":"9.2 Calculate Residuals","text":"predict energy weight, species, interaction using linear model. effect code species make output similar get ANOVA (really make sense treatment code , since neither cats ferrets meaningful \"baseline\").can use resid() function get residual error term model. difference predicted value (based weight species subject model parameters) actual value. values normally distributed.","code":"\n# effect-code species\ndata$species.e <- recode(data$species, cat = -0.5, ferret = 0.5)\n\nm1 <- lm(energy ~ weight*species.e, data = data)\nerr <- resid(m1)\n\nggplot() + geom_density(aes(err))"},{"path":"testing-for-normality.html","id":"shapiro-wilk","chapter":"9 Testing for normality","heading":"9.3 Shapiro-Wilk","text":"recommend using statistical tests normality. Essentially, underpowered small samples overpowered large samples. Robert Greener good discussion .. However, residuals \"pass\" Shapiro-Wilk normality test.","code":"\nshapiro.test(err)## \n##  Shapiro-Wilk normality test\n## \n## data:  err\n## W = 0.99579, p-value = 0.9905"},{"path":"testing-for-normality.html","id":"qq-plots","chapter":"9 Testing for normality","heading":"9.4 QQ plots","text":"better assess normality visually, quite hard judge normality density plot, especially small samples, can use QQ plot visualise close distribution normal. scatterplot created plotting two sets quantiles , used check data come specified distribution (normal distribution).data simulated, show almost perfect straight line. Real data always bit messier. even , points extremes often exactly line. takes practice tell QQ-plot shows clear signs non-normality.bimodal energy data good example QQ plot showing non-normal distribution (see points move away line ends), matter model .","code":"\n# ggplot function for more customisation\nqplot(sample = err) + \n  stat_qq_line(colour = \"dodgerblue\") +\n  labs(x = \"Theoretical distribution\",\n       y = \"Sample distribution\",\n       title = \"QQ Plot for Residual Error\")\nggplot(data, aes(sample = energy)) +\n  stat_qq() +\n  stat_qq_line(colour = \"dodgerblue\") +\n  labs(x = \"Theoretical distribution\",\n       y = \"Sample distribution\",\n       title = \"QQ Plot for Energy\")"},{"path":"testing-for-normality.html","id":"other-tests","chapter":"9 Testing for normality","heading":"9.5 Other tests","text":"get residuals tests? functions return models R resid() function. T-tests little trickier, can just convert GLM equivalents (Jonas Lindeløv great tutorial) use formulas .","code":"\n# simulated data to use below\nA <- rnorm(50, 0, 1)\nB <- rnorm(50, 0.5, 1)"},{"path":"testing-for-normality.html","id":"one-sample-t-test","chapter":"9 Testing for normality","heading":"9.5.1 One-sample t-test","text":"residuals one-samples t-test scores minus mean difference. (subtract mean difference, since distribution change add constant value.)","code":"\n# one-sample t-test against 0\nmu = 0\nt_o <- t.test(A, mu = mu)\nerr_t <- A - mean(A)\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to one-sample t-test\nm_o <- lm(A - mu ~ 1)\nerr_lm <- resid(m_o)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"paired-samples-t-test","chapter":"9 Testing for normality","heading":"9.5.2 Paired samples t-test","text":"residuals paired-samples t-test difference paired values, minus mean difference.","code":"\n# paired samples t-test\nt_p <- t.test(A, B, paired = TRUE)\ndiff <- A - B\nerr_t <- diff - mean(diff)\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to paired-samples t-test\nm_p <- lm(A-B ~ 1)\nerr_lm <- resid(m_p)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"independent-samples-t-test","chapter":"9 Testing for normality","heading":"9.5.3 Independent-samples t-test","text":"residuals independent-samples t-test scores minus group mean.","code":"\n# independent-sample t-test\nt_i <- t.test(A, B)\nerr_t <- c(A-mean(A), B-mean(B))\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to one-sample t-test\ndat <- data.frame(\n  val = c(A, B),\n  grp = rep(0:1, each = 50)\n)\n\nm_o <- lm(val ~ 1 + grp, dat)\nerr_lm <- resid(m_o)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"anova","chapter":"9 Testing for normality","heading":"9.5.4 ANOVA","text":"can use resid() function models output ANOVAs ANCOVAs.Dale Barr great blog post checking assumptions multilevel data.","code":"\nm_aov <- afex::aov_4(energy ~ weight*species.e + (1|id),\n  data = data,\n  factorize = FALSE\n)\nplot_aov <- qplot(sample = resid(m_aov)) + stat_qq_line()\n\nm_lm <- lm(energy ~ weight*species.e, data = data)\nplot_lm <- qplot(sample = resid(m_lm)) + stat_qq_line()\n\ncowplot::plot_grid(plot_aov, plot_lm, labels = c(\"aov\", \"lm\"))"},{"path":"testing-for-normality.html","id":"glossary","chapter":"9 Testing for normality","heading":"9.6 Glossary","text":"","code":""},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video .","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). install R, also install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page. install R, also install RTools; use \"recommended\" version highlighted near top list.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.3 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. Foe example, Lisa prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.4 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. course require make PDFs. generate PDF reports, additionally need install tinytex (Xie, 2021) run following code:","code":"\ntinytex::install_tinytex()"},{"path":"symbols.html","id":"symbols","chapter":"B Symbols","heading":"B Symbols","text":"\nFigure B.1: Image James Chapman/Soundimals\n","code":""},{"path":"conventions.html","id":"conventions","chapter":"C Conventions","heading":"C Conventions","text":"book use following conventions:Generic code: list(number = 1, letter = \"\")Highlighted code: dplyr::slice_max()File paths: data/sales.csvR Packages: tidyverseFunctions: paste()Strings: \"psyTeachR\"Numbers: 100, 3.14Logical values: TRUE, FALSEGlossary items: ordinalCitations: Wickham (2021)Internal links: Chapter 2External links: R Data ScienceMenu/interface options: New File...","code":""},{"path":"conventions.html","id":"webexercises","chapter":"C Conventions","heading":"C.1 Webexercises","text":"Type integer: going learn lot: TRUEFALSEWhat p-value?\n\nprobability null hypothesis truethe probability observed (extreme) data, assumption null-hypothesis truethe probability making error conclusion\nfound hidden text!","code":"\nprint(\"You found some hidden code!\")## [1] \"You found some hidden code!\""},{"path":"conventions.html","id":"alert-boxes","chapter":"C Conventions","heading":"C.2 Alert boxes","text":"Informational asides.Notes warn something.Notes things cause serious errors.Try .","code":""},{"path":"conventions.html","id":"code-chunks","chapter":"C Conventions","heading":"C.3 Code Chunks","text":"","code":"\n# code chunks\npaste(\"psyTeachR\", \"Tutorials\", 2021, sep = \" \")## [1] \"psyTeachR Tutorials 2021\"```{r setup, message = FALSE}\n# code chunks with visible r headers\nlibrary(tidyverse)```"},{"path":"conventions.html","id":"glossary-1","chapter":"C Conventions","heading":"C.4 Glossary","text":"","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit (PsyTeachR Team, 2021), provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"PsyTeachR Team. (2021). PsyTeachR tutorials. https://psyteachr.github.io/tutorials/Wickham, H. (2021). Tidyverse: Easily install load tidyverse. https://CRAN.R-project.org/package=tidyverseXie, Y. (2021). Tinytex: Helper functions install maintain tex live, compile latex documents. https://github.com/yihui/tinytex","code":""}]
