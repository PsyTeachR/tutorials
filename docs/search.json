[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"material tutorials governed CC-license. Feel free repurpose needs, please cite original ! source files can found https://github.com/psyteachr/tutorials.","code":""},{"path":"multiple-import.html","id":"multiple-import","chapter":"1 Importing data from multiple files","heading":"1 Importing data from multiple files","text":"Dale Barr (October 30, 2019)Sometimes multiple files, want read one big table analysis. following code allows read whole bunch files subdirectory.call subdirectory files live datadir; subdirectory may different name. files subdirectory, directory script, replace datadir full stop, .e., dir(\".\", \"\\\\.[Cc][Ss][Vv]$\").main work accomplished using map() function purrr package, part tidyverse. function map() applies function (case, read_csv()) elements vector supplied first argument.want run example see things work, need set environment sample files.example, filename variable todo contains values function read_csv() applied. argument col_types supplied map() intended passed along read_csv(). additional arguments want pass along read_csv() can placed (e.g., skip = 1).preprocessing need file reading , can write function call place read_csv().Sometimes filename contains metadata (e.g., string identifying subject: S01, S02 etc) want pull . can creating new variable using mutate() filename. example , strip away path filename (datadir/S01.csv S01.csv) using basename() remove file extension .csv using tools::file_path_sans_ext().","code":"\nlibrary(\"tidyverse\")\n\ndir.create(\"datadir\", FALSE)\n\n## create fake data files for three subjects\nwrite_csv(iris %>% slice(1:5), file.path(\"datadir\", \"S01.csv\"))\nwrite_csv(iris %>% slice(6:10), file.path(\"datadir\", \"S02.csv\"))\nwrite_csv(iris %>% slice(11:15), file.path(\"datadir\", \"S03.csv\"))\nlibrary(\"tidyverse\")\n\n# \"\\\\.csv$\" = find all files ending with csv or CSV\ntodo <- tibble(filename = dir(\"datadir\", \"\\\\.[Cc][Ss][Vv]$\", full.names = TRUE))\n\nall_data <- todo %>%\n  mutate(imported = map(filename, read_csv, col_types = \"ddddc\")) %>%\n  unnest(cols = c(imported))\n\nall_data\nall_data_id <- all_data %>%\n  mutate(subj_id = basename(filename) %>% tools::file_path_sans_ext()) %>%\n  select(subj_id, Sepal.Length:Species)\n\nall_data_id"},{"path":"multi-row-headers.html","id":"multi-row-headers","chapter":"2 Multi-Row Headers","heading":"2 Multi-Row Headers","text":"Lisa DeBruine (2021-10-17)student help forum asked help making wide-format dataset long. tried load data, realised first three rows header rows. code wrote deal .First, make small CSV \"file\" . typical case, read data file.try read data, get message duplicate column names resulting table \"fixed\" column headers next two columns headers first two rows.Instead, read just header rows setting n_max equal number header rows col_names FALSE.get table looks like :can make new header names pasting together names three rows summarising across columns paste() function collapsing using \"_\". Use unlist() unname() convert result table vector.Now can read data without three header rows. Use skip skip headers set col_names new names.excel file merges duplicate headers across rows, little trickier, still -able.first steps : read first three rows.code starts second column fills missing data value previous column.Now can continue generating pasted name .data set multiple headers, probably want change shape data. quick example use pivot_longer() pivot_wider() variable names like .","code":"\n# required packages\nlibrary(tidyverse)\nlibrary(readxl)\ndemo_csv <- \"SUB1, SUB1, SUB1, SUB1, SUB2, SUB2, SUB2, SUB2\nCOND1, COND1, COND2, COND2, COND1, COND1, COND2, COND2\nX, Y, X, Y, X, Y, X, Y\n10, 15, 6, 2, 42, 4, 32, 5\n4, 43, 7, 34, 56, 43, 2, 33\n77, 12, 14, 75, 36, 85, 3, 2\"\ndata <- read_csv(I(demo_csv))\ndata_head <- read_csv(demo_csv, \n                      n_max = 3, \n                      col_names = FALSE)\nnew_names <- data_head %>%\n  summarise(across(.fns = paste, collapse = \"_\")) %>%\n  unlist() %>% unname()\n\nnew_names## [1] \"SUB1_COND1_X\" \"SUB1_COND1_Y\" \"SUB1_COND2_X\" \"SUB1_COND2_Y\" \"SUB2_COND1_X\"\n## [6] \"SUB2_COND1_Y\" \"SUB2_COND2_X\" \"SUB2_COND2_Y\"\ndata <- read_csv(demo_csv, \n                 skip = 3, \n                 col_names = new_names,\n                 show_col_types = FALSE)\ndata_head <- read_excel(\"data/3headers_demo.xlsx\",\n                        n_max = 3, \n                        col_names = FALSE)\nfor (i in 2:ncol(data_head)) {\n  prev <- data_head[, i-1]\n  this <- data_head[, i]\n  missing <- is.na(this)\n  this[missing, ] <- prev[missing, ]\n  data_head[, i] <- this\n}\nnew_names <- data_head %>%\n  summarise(across(.fns = paste, collapse = \"_\")) %>%\n  unlist() %>% unname()\n\nnew_names## [1] \"SUB1_COND1_X\" \"SUB1_COND1_Y\" \"SUB1_COND2_X\" \"SUB1_COND2_Y\" \"SUB2_COND1_X\"\n## [6] \"SUB2_COND1_Y\" \"SUB2_COND2_X\" \"SUB2_COND2_Y\"\ndata <- read_excel(\"data/3headers_demo.xlsx\", \n                   skip = 3, \n                   col_names = new_names)\n\ndata_long <- data %>%\n  # add a row ID column if one doesn't exist already\n  mutate(trial_id = row_number()) %>%\n  # make a row for each data column\n  pivot_longer(\n    cols = -trial_id, # everything except trial_id\n    names_to = c(\"sub_id\", \"condition\", \"coord\"),\n    names_sep = \"_\",\n    values_to = \"val\"\n  ) %>%\n  # make x and y coord columns\n  pivot_wider(\n    names_from = coord,\n    values_from = val\n  )"},{"path":"detecting-runs-in-a-sequence.html","id":"detecting-runs-in-a-sequence","chapter":"3 Detecting \"runs\" in a sequence","heading":"3 Detecting \"runs\" in a sequence","text":"Dale Barr (October 30, 2019)say table like , want find start end frames run Z amidst , b, c, d. code sets kind situation. worry understand code; just run create example data runsdata, look table.say want find start stop frames Z appears stimulus, independently combination subject trial. stimulus looks subject 1 trial 1.can see first run Zs frame 11 13, 27 second 25 27. want write function processes data trial results table like :first thing add logical vector tibble whose value TRUE target value (e.g., Z) present sequence, false otherwise.want iterate subjects trials. start creating tibble columns is_target nested column called subtbl.want iterate little subtables stored within subtbl row table, passing table function find runs return another table, store new column. write function detect runs. function need function rle() (Run-Length Encoding) base R. run logical vector created (is_target). creating function, see rle() values is_target subject 1, trial 1.make sense, look help rle() (type ?rle console). Now ready write function, detect_runs().can test function s1t1 just make sure works.OK, now ready run function.Now just unnest done!","code":"\nlibrary(\"tidyverse\")\n\ncreate_run_vec <- function() {\n  ## create a random string of letters with two runs\n  c(rep(sample(letters[1:4]), sample(2:4, 4, TRUE)),\n               rep(\"Z\", 3),\n               rep(sample(letters[1:4]), sample(2:4, 4, TRUE)),\n               rep(\"Z\", 3),\n               rep(sample(letters[1:4], 2), sample(2:4, 2, TRUE)))\n}\n\n## 5 subjects, 3 trials each\nrunsdata <- tibble(\n  subject = rep(1:5, each = 3),\n  trial = rep(1:3, 5),\n  stimulus = rerun(15, create_run_vec())) %>%\n  unnest(stimulus) %>%\n  group_by(subject, trial) %>%\n  ungroup() %>%\n  select(subject, trial, stimulus)##  [1] \"c\" \"c\" \"a\" \"a\" \"b\" \"b\" \"b\" \"d\" \"d\" \"d\" \"Z\" \"Z\" \"Z\" \"d\" \"d\" \"c\" \"c\" \"c\" \"c\"\n## [20] \"a\" \"a\" \"b\" \"b\" \"b\" \"Z\" \"Z\" \"Z\" \"b\" \"b\" \"b\" \"b\" \"d\" \"d\" \"d\" \"d\"\nrunsdata_tgt <- runsdata %>%\n  mutate(is_target = (stimulus == \"Z\"))\n\nhead(runsdata_tgt)\nruns_nest <- runsdata_tgt %>%\n  select(-stimulus) %>% # don't need it anymore\n  nest(subtbl = c(is_target))\ns1t1 <- runsdata_tgt %>% filter(subject == 1L, trial == 1L) %>% pull(is_target)\n\ns1t1\n\nrle(s1t1)##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n## [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [25]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## Run Length Encoding\n##   lengths: int [1:5] 10 3 11 3 8\n##   values : logi [1:5] FALSE TRUE FALSE TRUE FALSE\ndetect_runs <- function(x) {  \n  if (!is.logical(x[[1]])) stop(\"'x' must be a tibble whose first column is of type 'logical'\")\n  runs <- rle(x[[1]])\n  run_start_fr <- c(1L, cumsum(runs$lengths[-length(runs$lengths)]) + 1L)\n  run_end_fr <- run_start_fr + (runs$lengths - 1L)\n  \n  tgt_start <- run_start_fr[runs$values]\n  tgt_end <- run_end_fr[runs$value]\n  tibble(run = seq_along(tgt_start),\n         start_fr = tgt_start,\n         end_fr = tgt_end)\n}\ndetect_runs(tibble(lvec = s1t1))\nresult <- runs_nest %>%\n  mutate(runstbl = map(subtbl, detect_runs))\n\nhead(result)\ndata <- result %>%\n  select(-subtbl) %>%\n  unnest(runstbl)\n\nhead(data)"},{"path":"pipes.html","id":"pipes","chapter":"4 Pipes","heading":"4 Pipes","text":"Lisa DeBruine (2018-12-22)Pipes way order code readable format.say small data table 10 participant IDs, two columns variable type , 2 columns variable type B. want calculate mean variables mean B variables return table 10 rows (1 participant) 3 columns (id, A_mean B_mean).One way creating new object every step using object next step. pretty clear, created 6 unnecessary data objects environment. can get confusing long scripts.\ncan name object data keep replacing old data object new one step. keep environment clean, recommend makes easy accidentally run code order running line--line development debugging.\nOne way avoid extra objects nest functions, literally replacing data object code generated previous step. can fine short chains.gets extremely confusing long chains:pipe lets \"pipe\" result function next function, allowing put code logical order without creating many extra objects.can read code top bottom follows:Make tibble called data \nid 1 10,\nA1 10 random numbers normal distribution mean 0,\nA2 10 random numbers normal distribution mean 1,\nB1 10 random numbers normal distribution mean 2,\nB2 10 random numbers normal distribution mean 3; \nid 1 10,A1 10 random numbers normal distribution mean 0,A2 10 random numbers normal distribution mean 1,B1 10 random numbers normal distribution mean 2,B2 10 random numbers normal distribution mean 3; thenGather create variable value column columns A_1 B_2; thenSeparate column variable 2 new columns called varand var_n, separate character 1; thenGroup columns id var; thenSummarise new column called mean mean value column group, drop groups ; thenSpread make new columns key names var values mean; thenRename make columns called A_mean (old ) B_mean (old B)can make intermediate objects whenever need break code getting complicated need debug something.\ncan debug pipe running just first functions highlighting beginning just pipe want stop . Try highlighting data <- end separate function typing cmd-return. data look like now?\n","code":"\nlibrary(tidyverse)\n# make a data table with 10 subjects\ndata_original <- tibble(\n  id = 1:10,\n  A1 = rnorm(10, 0),\n  A2 = rnorm(10, 1),\n  B1 = rnorm(10, 2),\n  B2 = rnorm(10, 3)\n)\n\n# gather columns A1 to B2 into \"variable\" and \"value\" columns\ndata_gathered <- gather(data_original, variable, value, A1:B2)\n\n# separate the variable column at the _ into \"var\" and \"var_n\" columns\ndata_separated <- separate(data_gathered, variable, c(\"var\", \"var_n\"), sep = 1)\n\n# group the data by id and var\ndata_grouped <- group_by(data_separated, id, var)\n\n# calculate the mean value for each id/var \ndata_summarised <- summarise(data_grouped, mean = mean(value), .groups = \"drop\")\n\n# spread the mean column into A and B columns\ndata_spread <- spread(data_summarised, var, mean)\n\n# rename A and B to A_mean and B_mean\ndata <- rename(data_spread, A_mean = A, B_mean = B)\nmean_petal_width <- round(mean(iris$Petal.Width), 2)\n# do not ever do this!!\ndata <- rename(\n  spread(\n    summarise(\n      group_by(\n        separate(\n          gather(\n            tibble(\n              id = 1:10,\n              A1 = rnorm(10, 0),\n              A2 = rnorm(10, 1),\n              B1 = rnorm(10, 2),\n              B2 = rnorm(10, 3)), \n            variable, value, A1:B2), \n          variable, c(\"var\", \"var_n\"), sep = 1), \n        id, var), \n      mean = mean(value),\n      .groups = \"drop\"), \n    var, mean), \n  A_mean = A, B_mean = B)\n# calculate mean of A and B variables for each participant\ndata <- tibble(\n  id = 1:10,\n  A1 = rnorm(10, 0),\n  A2 = rnorm(10, 1),\n  B1 = rnorm(10, 2),\n  B2 = rnorm(10, 3)\n) %>%\n  gather(variable, value, A1:B2) %>%\n  separate(variable, c(\"var\", \"var_n\"), sep=1) %>%\n  group_by(id, var) %>%\n  summarise(mean = mean(value),\n            .groups = \"drop\") %>%\n  spread(var, mean) %>%\n  rename(A_mean = A, B_mean = B)"},{"path":"converting-strings-to-numbers.html","id":"converting-strings-to-numbers","chapter":"5 Converting strings to numbers","heading":"5 Converting strings to numbers","text":"Dale Barr (August 18, 2021)common situation want convert string values (\"Almost Always\", \"Frequently\") numeric values (5, 4, etc) can calculate score.solution present use something called \"lookup table\" matches string values numbers.First, assume following (made-) questionnaire asks four questions hygiene. bathe, often :Wash legs?Wash behind ears?Wash toes?Shampoo hair?assume answers scale made values \"Never\", \"Sometimes\", \"Frequently\", \"Always\", want assign scores 0, 1, 2, 3 respectively.(also made-) data five participants, stored tibble named dat.(want make dat can follow along example running code R, click button .)data wide format: single row participant data question forming columns. going first convert data wide long using pivot_longer(). (see minute.)Take look dat_long.Now one variable need convert numeric values (response) instead original four. easy solution: create \"lookup table\" mapping string values numeric values like .first check unique string values data. lookup table must match exactly approach work.step important sometimes fields can special characters normally see print table. distinct() %>% pull() pattern give values way makes visible. instance, student values data like :lookup table constantly failing lookup table created \\n middle string. Computers literal!OK now ready create lookup table match four values numbers.final step, inner_join() original table dat_long lookup variable response.IMPORTANT: check make sure join worked intended. values lookup table must exactly match values response column dat_long. easy make typo lookup table, values lost. easy test make sure number rows joined matches number rows dat_long.function stopifnot() make script fail stated condition (tables number rows) satisfied.Uh oh. Running gives Error: nrow(joined) == nrow(dat_long) TRUE. test failed, deliberately included typo lookup table. Can see ?always Always. Capitalization matters!fix lookup table good go. full code demonstration:useto calculate score subject.","code":"\nlibrary(\"tidyverse\")\n\ndat <- tribble(\n  ~subj_id, ~wash_legs,   ~wash_ears,   ~wash_toes,   ~shampoo,\n  \"S01\",    \"Sometimes\",  \"Never\",      \"Never\",      \"Frequently\",\n  \"S02\",    \"Sometimes\",  \"Frequently\", \"Frequently\", \"Always\",\n  \"S03\",    \"Never\",      \"Never\",      \"Never\",      \"Frequently\",\n  \"S04\",    \"Always\",     \"Always\",     \"Sometimes\",  \"Always\",\n  \"S05\",    \"Frequently\", \"Sometimes\",  \"Never\",      \"Sometimes\")\ndat_long <- dat %>%\n  pivot_longer(cols = wash_legs:shampoo,\n               names_to = \"question\", values_to = \"response\")\ndat_long %>%\n  distinct(response) %>%\n  pull()## [1] \"Sometimes\"  \"Never\"      \"Frequently\" \"Always\"[1] \"Somewhat\\nInfrequently\" \"Somewhat\\nFrequently\"   \"Very\\nInfrequently\"\n[4] \"Almost\\nNever\"          \"Very\\nFrequently\"       \"Almost\\nAlways\"\nlookup <- tribble(\n  ~response, ~score,\n  \"Never\",      0,\n  \"Sometimes\",  1,\n  \"Frequently\", 2,\n  \"always\",     3)\njoined <- inner_join(dat_long, lookup, by = \"response\")\nstopifnot(nrow(joined) == nrow(dat_long))\ndat_long <- dat %>%\n  pivot_longer(cols = wash_legs:shampoo,\n               names_to = \"question\", values_to = \"response\")\n\n## check for hidden values\ndat_long %>%\n  distinct(response) %>%\n  pull()\n\nlookup <- tribble(\n  ~response, ~score,\n  \"Never\",      0,\n  \"Sometimes\",  1,\n  \"Frequently\", 2,\n  \"Always\",     3)\n\njoined <- inner_join(dat_long, lookup, by = \"response\")\n\n## test whether the number of rows match\nstopifnot(nrow(joined) == nrow(dat_long))\njoined %>%\n  group_by(subj_id) %>%\n  summarise(hygiene = sum(score))"},{"path":"reverse-scoring.html","id":"reverse-scoring","chapter":"6 Reverse scoring","heading":"6 Reverse scoring","text":"Dale Barr (January 2021)Sometimes necessary reverse scores given questionnaire items. instance, scale Dog Appreciation Scale (DAS) [just made ] might items 'strongly agree' associated appreciating dogs (given highest score) items associated loathing (given lowest).say DAS scale six items measuring dog appreciation. People respond six items 5 point likert scale, 1=strongly disagree, 2=somewhat disagree, 3=neutral, 4=somewhat agree, 5=strongly agree.\nTable 6.1: Dog Appreciation Scale\nmade-questionnaire data 6 items 3 subjects, contained tibble named das. want reverse score items \"Cats better dogs\", \"Dogs noisy\", \"Dogs much responsibility\" summing total subject.First, assume data long format, like table . , please see materials reshaping wide long, section MSC book.going use programming trick call \"N-plus-one-minus-X trick\" score items need reverse coded. trick work whenever scale N scale points goes integer steps 1 N (e.g., 1, 2, 3, 4, 5). subtract Xs (observed score) N+1 get reversed value.newscore = (number_of_scale_points + 1) - oldscoreSo 5 point scale, :newscore = 6 - oldscoreand 7 point scale isnewscore = 8 - oldscore.can see works using following code:Note: scale goes 0 N, use N - X rather (N + 1) - X reverse score.can see already need something like:items need reverse scored. if_else() comes . , better said, if_else() comes %% (can pardon bit R humor).code adds new variable newscore result if_else() command stores resulting table das_coded. command following syntax:if_else(condition, value_if_true, value_if_false)., current value item found within vector options (%% operator ), first expression evaluates TRUE, 6-score returned; first expression evaluates FALSE, score returned.whenever recode score variable, ALWAYS check code correct, typos likely. best way run little test console. can just print data das_coded, lot data, use distinct() look check distinct values observed data.can see \"Cats better dogs\" \"Dogs noisy\" successfully reverse scored. can also see items forward scored, e.g., \"like dogs\", indeed forward scored (scores change).reverse scoring \"Dogs much responsibility\" failed. Can see problem code (hint: typo).responsibility mistyped responsibilitiySo correct code :done ! Now can proceed analyze data .","code":"\nlibrary(\"tidyverse\")\n\ndas <- tribble(\n  ~subj_id, ~item, ~score,\n  \"S01\", \"I like dogs\",                      5,\n  \"S01\", \"Dogs are fun\",                     5,\n  \"S01\", \"Cats are better than dogs\",        1,\n  \"S01\", \"Dogs are helpful\",                 4,\n  \"S01\", \"Dogs are too noisy\",               2,\n  \"S01\", \"Dogs are too much responsibility\", 2,\n  \"S02\", \"I like dogs\",                      3,\n  \"S02\", \"Dogs are fun\",                     4,\n  \"S02\", \"Cats are better than dogs\",        2,\n  \"S02\", \"Dogs are helpful\",                 4,\n  \"S02\", \"Dogs are too noisy\",               3,\n  \"S02\", \"Dogs are too much responsibility\", 5,\n  \"S03\", \"I like dogs\",                      1,\n  \"S03\", \"Dogs are fun\",                     3,\n  \"S03\", \"Cats are better than dogs\",        5,\n  \"S03\", \"Dogs are helpful\",                 2,\n  \"S03\", \"Dogs are too noisy\",               4,\n  \"S03\", \"Dogs are too much responsibility\", 5)\noldscores <- 1:5\nnewscores <- 6 - oldscores\n\nrbind(oldscores, newscores)##           [,1] [,2] [,3] [,4] [,5]\n## oldscores    1    2    3    4    5\n## newscores    5    4    3    2    1\ndas %>%\n  mutate(newscore = 6 - score)\ndas_coded <- das %>%\n  mutate(newscore = if_else(item %in% c(\"Cats are better than dogs\",\n                                        \"Dogs are too noisy\",\n                                        \"Dogs are too much responsibilitiy\"),\n                            6 - score,\n                            score))\ndas_coded %>%\n  distinct(item, score, newscore) %>%\n  print(n = +Inf) ## this makes sure *all* rows are printed, not just the first 20## # A tibble: 16 × 3\n##    item                             score newscore\n##    <chr>                            <dbl>    <dbl>\n##  1 I like dogs                          5        5\n##  2 Dogs are fun                         5        5\n##  3 Cats are better than dogs            1        5\n##  4 Dogs are helpful                     4        4\n##  5 Dogs are too noisy                   2        4\n##  6 Dogs are too much responsibility     2        2\n##  7 I like dogs                          3        3\n##  8 Dogs are fun                         4        4\n##  9 Cats are better than dogs            2        4\n## 10 Dogs are too noisy                   3        3\n## 11 Dogs are too much responsibility     5        5\n## 12 I like dogs                          1        1\n## 13 Dogs are fun                         3        3\n## 14 Cats are better than dogs            5        1\n## 15 Dogs are helpful                     2        2\n## 16 Dogs are too noisy                   4        2\ndas_coded <- das %>%\n  mutate(newscore = if_else(item %in% c(\"Cats are better than dogs\",\n                                        \"Dogs are too noisy\",\n                                        \"Dogs are too much responsibility\"),\n                            6 - score,\n                            score))\ndas_coded %>%\n  distinct(item, score, newscore) %>%\n  print(n = +Inf) ## this makes sure *all* rows are printed, not just the first 20## # A tibble: 16 × 3\n##    item                             score newscore\n##    <chr>                            <dbl>    <dbl>\n##  1 I like dogs                          5        5\n##  2 Dogs are fun                         5        5\n##  3 Cats are better than dogs            1        5\n##  4 Dogs are helpful                     4        4\n##  5 Dogs are too noisy                   2        4\n##  6 Dogs are too much responsibility     2        4\n##  7 I like dogs                          3        3\n##  8 Dogs are fun                         4        4\n##  9 Cats are better than dogs            2        4\n## 10 Dogs are too noisy                   3        3\n## 11 Dogs are too much responsibility     5        1\n## 12 I like dogs                          1        1\n## 13 Dogs are fun                         3        3\n## 14 Cats are better than dogs            5        1\n## 15 Dogs are helpful                     2        2\n## 16 Dogs are too noisy                   4        2"},{"path":"highlight-a-range-of-x-values.html","id":"highlight-a-range-of-x-values","chapter":"7 Highlight a range of x-values","heading":"7 Highlight a range of x-values","text":"Dale Barr (March 23, 2020)Sometimes want highlight particular range values; example, particular period time time series.code used create following plot.\nFigure 7.1: time series x = 40-60 highlighted\n","code":"\nlibrary(\"tidyverse\")\n\n## make up some example data\nexdata <- tibble(x = rep(1:100, 2),\n                 series = rep(1:2, each = 100),\n                 y = rnorm(200) + rep(c(30, 50), each = 100))\n\n## region we want to highlight\nregions <- tibble(x1 = 40, x2 = 60, y1 = -Inf, y2 = +Inf)\n\nggplot(exdata, aes(x, y)) +\n  geom_rect(data = regions,\n            inherit.aes = FALSE,\n            mapping = aes(xmin = x1, xmax = x2,\n                          ymin = y1, ymax = y2),\n            color = \"transparent\",\n            fill = \"blue\",\n            alpha = .2) +\n  geom_line(aes(group = series))"},{"path":"testing-for-normality.html","id":"testing-for-normality","chapter":"8 Testing for normality","heading":"8 Testing for normality","text":"Lisa DeBruine (2021-02-01)probably directed asked someone test normality predictors analysis. However, statistical tests like t-tests, ANOVAs, GLM-based tests assume residuals normally distributed matter predictors even dependent variable .blog post, going use data simulation show can visualise normality residuals QQ plots. going simulate data totally hypothetical population ferrets cats. going try predict energy levels pets weight. limited experience, tiny ferrets way energetic big ferrets. know nothing cats.Tiny, energetic Darwin big, lazy brother, Oy","code":"\nlibrary(tidyverse) # for data wrangling\nlibrary(faux)      # for data simulation\nlibrary(afex)      # for anova\nlibrary(cowplot)   # for dataviz\nset.seed(8675309)  # to make sure simulation values don't vary between runs"},{"path":"testing-for-normality.html","id":"simulate-data","chapter":"8 Testing for normality","heading":"8.1 Simulate Data","text":"use faux simulate data based data parameters like means, SDs correlations group. moment, faux can simulate multivariate normal distributions can convert distributions. simulate weights normal distribution mean 0 SD 1, convert uniform distribution pet type based ranges found online. Energy simulated normal distributions different means SDs cats ferrets. Energy uncorrelated weight cats negatively correlated ferrets.N.B. used simulating data using model parameters, way might make sense , often difficult figure parameters already pilot data.weight bimodal made two uniform distributions, energy bimodal made two normal distributions.\nFigure 8.1: Distibutions overall within species.\nrun Shapiro-Wilk test variables, conclude definitely normally distributed, matter !","code":"\ndata <- faux::sim_design(\n  within = list(vars = c(\"weight\", \"energy\")),\n  between = list(species = c(\"cat\", \"ferret\")),\n  n = 50,\n  mu = list(weight = c(cat = 0, ferret = 0),\n            energy = c(cat = 50, energy = 100)),\n  sd = list(weight = c(cat = 1, ferret = 1),\n            energy = c(cat = 15, energy = 20)),\n  r = list(cat = 0, ferret = -0.5),\n  plot = FALSE\n) %>%\n  mutate(weight = case_when(\n    species == \"cat\" ~ norm2unif(weight, 3.6, 4.5),\n    species == \"ferret\" ~ norm2unif(weight, 0.7, 2.0)\n  ))\nn <- 50\n\n# values approximated from an lm analysis\nb_0  <-  92 # intercept\nb_w  <- -13 # fixed effect of weight\nb_s  <-  85 # fixed effect of species\nb_ws <- -26 # weight*species interaction\nerr_sd <- 16 # SD of error term\n\n# simulate populations of cats and ferrets \n# with weights from uniform distributions\ncat <- data.frame(\n  id = paste0(\"C\", 1:n),\n  species = \"cat\",\n  weight = runif(n, 3.6, 4.5)\n)\n\nferret <- data.frame(\n  id = paste0(\"F\", 1:n),\n  species = \"ferret\",\n  weight = runif(n, 0.7, 2.0)\n)\n\n# join data and calculate DV based on GLM\ndata <- bind_rows(cat, ferret) %>%\n  mutate(\n    # effect-code species\n    species.e = recode(species, cat = -0.5, ferret = 0.5),\n    # simulate error term\n    err = rnorm(2*n, 0, err_sd),\n    # calculate DV\n    energy = b_0 + species.e*b_s + weight*b_w + \n             species.e*weight*b_ws + err\n  )\nshapiro.test(data$energy)## \n##  Shapiro-Wilk normality test\n## \n## data:  data$energy\n## W = 0.95486, p-value = 0.001759\nshapiro.test(data$weight)## \n##  Shapiro-Wilk normality test\n## \n## data:  data$weight\n## W = 0.82694, p-value = 1.821e-09"},{"path":"testing-for-normality.html","id":"calculate-residuals","chapter":"8 Testing for normality","heading":"8.2 Calculate Residuals","text":"predict energy weight, species, interaction using linear model. effect code species make output similar get ANOVA (really make sense treatment code , since neither cats ferrets meaningful \"baseline\").can use resid() function get residual error term model. difference predicted value (based weight species subject model parameters) actual value. values normally distributed.","code":"\n# effect-code species\ndata$species.e <- recode(data$species, cat = -0.5, ferret = 0.5)\n\nm1 <- lm(energy ~ weight*species.e, data = data)\nerr <- resid(m1)\n\nggplot() + geom_density(aes(err))"},{"path":"testing-for-normality.html","id":"shapiro-wilk","chapter":"8 Testing for normality","heading":"8.3 Shapiro-Wilk","text":"recommend using statistical tests normality. Essentially, underpowered small samples overpowered large samples. Robert Greener good discussion .. However, residuals \"pass\" Shapiro-Wilk normality test.","code":"\nshapiro.test(err)## \n##  Shapiro-Wilk normality test\n## \n## data:  err\n## W = 0.99579, p-value = 0.9905"},{"path":"testing-for-normality.html","id":"qq-plots","chapter":"8 Testing for normality","heading":"8.4 QQ plots","text":"better assess normality visually, quite hard judge normality density plot, especially small samples, can use QQ plot visualise close distribution normal. scatterplot created plotting two sets quantiles , used check data come specified distribution (normal distribution).data simulated, show almost perfect straight line. Real data always bit messier. even , points extremes often exactly line. takes practice tell QQ-plot shows clear signs non-normality.bimodal energy data good example QQ plot showing non-normal distribution (see points move away line ends), matter model .","code":"\n# ggplot function for more customisation\nqplot(sample = err) + \n  stat_qq_line(colour = \"dodgerblue\") +\n  labs(x = \"Theoretical distribution\",\n       y = \"Sample distribution\",\n       title = \"QQ Plot for Residual Error\")\nggplot(data, aes(sample = energy)) +\n  stat_qq() +\n  stat_qq_line(colour = \"dodgerblue\") +\n  labs(x = \"Theoretical distribution\",\n       y = \"Sample distribution\",\n       title = \"QQ Plot for Energy\")"},{"path":"testing-for-normality.html","id":"other-tests","chapter":"8 Testing for normality","heading":"8.5 Other tests","text":"get residuals tests? functions return models R resid() function. T-tests little trickier, can just convert GLM equivalents (Jonas Lindeløv great tutorial) use formulas .","code":"\n# simulated data to use below\nA <- rnorm(50, 0, 1)\nB <- rnorm(50, 0.5, 1)"},{"path":"testing-for-normality.html","id":"one-sample-t-test","chapter":"8 Testing for normality","heading":"8.5.1 One-sample t-test","text":"residuals one-samples t-test scores minus mean difference. (subtract mean difference, since distribution change add constant value.)","code":"\n# one-sample t-test against 0\nmu = 0\nt_o <- t.test(A, mu = mu)\nerr_t <- A - mean(A)\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to one-sample t-test\nm_o <- lm(A - mu ~ 1)\nerr_lm <- resid(m_o)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"paired-samples-t-test","chapter":"8 Testing for normality","heading":"8.5.2 Paired samples t-test","text":"residuals paired-samples t-test difference paired values, minus mean difference.","code":"\n# paired samples t-test\nt_p <- t.test(A, B, paired = TRUE)\ndiff <- A - B\nerr_t <- diff - mean(diff)\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to paired-samples t-test\nm_p <- lm(A-B ~ 1)\nerr_lm <- resid(m_p)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"independent-samples-t-test","chapter":"8 Testing for normality","heading":"8.5.3 Independent-samples t-test","text":"residuals independent-samples t-test scores minus group mean.","code":"\n# independent-sample t-test\nt_i <- t.test(A, B)\nerr_t <- c(A-mean(A), B-mean(B))\nplot_t <- qplot(sample = err_t) + stat_qq_line()\n\n# lm equivalent to one-sample t-test\ndat <- data.frame(\n  val = c(A, B),\n  grp = rep(0:1, each = 50)\n)\n\nm_o <- lm(val ~ 1 + grp, dat)\nerr_lm <- resid(m_o)\nplot_lm <- qplot(sample = err_lm) + stat_qq_line()\n\ncowplot::plot_grid(plot_t, plot_lm, labels = c(\"t\", \"lm\"))"},{"path":"testing-for-normality.html","id":"anova","chapter":"8 Testing for normality","heading":"8.5.4 ANOVA","text":"can use resid() function models output ANOVAs ANCOVAs.Dale Barr great blog post checking assumptions multilevel data.","code":"\nm_aov <- afex::aov_4(energy ~ weight*species.e + (1|id),\n  data = data,\n  factorize = FALSE\n)\nplot_aov <- qplot(sample = resid(m_aov)) + stat_qq_line()\n\nm_lm <- lm(energy ~ weight*species.e, data = data)\nplot_lm <- qplot(sample = resid(m_lm)) + stat_qq_line()\n\ncowplot::plot_grid(plot_aov, plot_lm, labels = c(\"aov\", \"lm\"))"},{"path":"testing-for-normality.html","id":"glossary","chapter":"8 Testing for normality","heading":"8.6 Glossary","text":"","code":""},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video .","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). install R, also install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page. install R, also install RTools; use \"recommended\" version highlighted near top list.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.3 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. Foe example, Lisa prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.4 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. course require make PDFs. generate PDF reports, additionally need install tinytex (Xie, 2021) run following code:","code":"\ntinytex::install_tinytex()"},{"path":"symbols.html","id":"symbols","chapter":"B Symbols","heading":"B Symbols","text":"\nFigure B.1: Image James Chapman/Soundimals\n","code":""},{"path":"conventions.html","id":"conventions","chapter":"C Conventions","heading":"C Conventions","text":"book use following conventions:Generic code: list(number = 1, letter = \"\")Highlighted code: dplyr::slice_max()File paths: data/sales.csvR Packages: tidyverseFunctions: paste()Strings: \"psyTeachR\"Numbers: 100, 3.14Logical values: TRUE, FALSEGlossary items: ordinalCitations: Wickham (2021)Internal links: Chapter 1External links: R Data ScienceMenu/interface options: New File...","code":""},{"path":"conventions.html","id":"webexercises","chapter":"C Conventions","heading":"C.1 Webexercises","text":"Type integer: going learn lot: TRUEFALSEWhat p-value?\n\nprobability null hypothesis truethe probability observed (extreme) data, assumption null-hypothesis truethe probability making error conclusion\nfound hidden text!","code":"\nprint(\"You found some hidden code!\")## [1] \"You found some hidden code!\""},{"path":"conventions.html","id":"alert-boxes","chapter":"C Conventions","heading":"C.2 Alert boxes","text":"Informational asides.Notes warn something.Notes things cause serious errors.Try .","code":""},{"path":"conventions.html","id":"code-chunks","chapter":"C Conventions","heading":"C.3 Code Chunks","text":"","code":"\n# code chunks\npaste(\"psyTeachR\", \"Tutorials\", 2021, sep = \" \")## [1] \"psyTeachR Tutorials 2021\"```{r setup, message = FALSE}\n# code chunks with visible r headers\nlibrary(tidyverse)```"},{"path":"conventions.html","id":"glossary-1","chapter":"C Conventions","heading":"C.4 Glossary","text":"","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit (Team, 2021), provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
